ContextTune — Executive Summary

What this project does
- Tunes per-repo guidance text blocks via hill-climbing, then evaluates
  on SWE-bench Verified under two conditions (no_context vs tuned_context).
- Uses SWE-smith to generate training tasks for each repo.
- Keeps SWE-bench harness unchanged; all outputs are standard predictions
  JSONL + harness result files.

Architecture
- One tunable text block per repo (RepoGuidance dataclass, ≤3200 chars).
- Hill-climbing tuner: init G₀ from LLM + repo introspection, then
  T iterations × K candidates, scored on N SWE-smith tasks.
- Scoring runs the Docker mini-swe-agent-swebench runner.
- Final evaluation on SWE-bench Verified (no_context vs tuned_context).

Pipeline
1. Generate SWE-smith tasks per repo (scripts/generate_swesmith_tasks.py).
2. Tune guidance per repo (scripts/tune_single_repo.py or run_experiment.py).
3. Evaluate on Verified (scripts/run_experiment.py Phase 2).

Key modules
- context_policy/guidance/ — schema, init, propose, score, tuner, gating, repo_info.
- context_policy/loop/orchestrator.py — experiment orchestrator (tuning + eval).
- context_policy/runner/mini_swe_agent_swebench.py — Docker-native agent runner.
- context_policy/prompting/prompt_builder.py — prompt construction with guidance injection.

Context injection format
  # REPO GUIDANCE (AUTO-TUNED)
  ...guidance lines...
  # END REPO GUIDANCE

12 experiment repos
  django, astropy, sympy, scikit-learn, matplotlib, flask,
  sphinx, pylint, pytest, requests, xarray, seaborn.

Runtime
- Primary: Linux/WSL2 with Docker.
- Model inference: OpenAI-compatible HTTP API (OPENAI_BASE_URL, OPENAI_API_KEY).
- Runner: mini_swe_agent_swebench (Docker) for both tuning and final eval.
